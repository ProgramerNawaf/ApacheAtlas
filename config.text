from datetime import datetime
import json
import os
import re
import subprocess

import numpy as np
import requests
from classes.AzureDevops import AzureDevops
from classes.Connection import Connection
import pandas as pd
import oracledb
from requests.auth import HTTPBasicAuth




def add_change_records(df, columns, cr_detection, change_type):
    """
    Process dataframe records and add to CR_DETECTION.
    
    Args:
        df: pandas DataFrame to process
        columns: list of column names to extract
        cr_detection: CR_DETECTION dictionary to update
        change_type: key in CR_DETECTION dict (default: 'Change Attribute')
    """
    if df.empty:
        return
    
    records = (
        df[columns]
        .drop_duplicates()
        .replace({np.nan: None})
        .to_dict('records')
    )
    
    if records:
        cr_detection[change_type].extend(records)
        cr_detection['Logs']['total_changes'] += len(records)
        

            

if __name__ == '__main__':

    CR_DETECTION = {
        "Add Table": [],
        "Delete Table": [],
        "Change Table":[],
        "Add Attribute": [],
        "Delete Attribute": [],
        "Change Attribute":[],
        "Logs": {
            "detection_time": datetime.now().isoformat(),
            "total_changes": 0
        },
        "Impacted Pipliens": []
    }
#     # customMetaDataCatlogDatabase=Connection(option='Destination',databaseType='SQL')
#     # customMetaDataCatlogDatabase.connect()

#     # print('testing connection destinationDatabase ',customMetaDataCatlogDatabase.testConnection())

# #     query=f'''
# #                SELECT
# #                 DA.ID as DATASTORE_ID , PP.ID as PIPELINE_ID , P.ID as ATTRIBUTE_ID , t.Connection_ID as CONNECTION_ID , 
# #                 T.Schema_Name as TABLE_SCHEMA,
# #                 T.Table_Name as TABLE_NAME,
# #                 P.Attribute_Name as ATTRIBUTE_NAME,
# #                 P.Attribute_Type as DATA_TYPE,
# #                 CASE WHEN P.IsPK = 1 THEN 'Yes' WHEN P.IsPK = 0 THEN 'No' END AS IS_PK,
# #                 CASE WHEN P.IsFK = 1 THEN 'Yes' WHEN P.IsFK = 0 THEN 'No' END AS IS_FK,
# #                 CASE WHEN P.IsLastOperation_Attribute = 1 THEN 'Yes' END AS IsLastOperation_Attribute,
# #                 CASE WHEN P.IsTimestamp_Attribute = 1 THEN 'Yes' END AS IsTimestamp_Attribute,
# #                 P.Attribute_Format,
# #                 RA.Referenced_Schema,
# #                 RA.Referenced_Tables,
# #                 RA.Referenced_Attributes,
# #                 CASE WHEN P.IsMasked = 1 THEN 'Yes' WHEN P.IsMasked = 0 THEN 'No' END AS IsMasked
# #                 FROM Meta_Agency A
# #                 LEFT JOIN Meta_Dataset D ON A.ID = D.Agency_ID
# #                 LEFT JOIN Meta_Datastore DA ON DA.Dataset_ID = D.ID
# #                 LEFT JOIN Meta_Pipeline PP ON DA.ID = PP.Datastore_ID
# #                 LEFT JOIN Meta_Attribute P ON P.Datastore_ID = DA.ID
# #                 LEFT JOIN Meta_Table T ON T.Datastore_ID = DA.ID
# #                 LEFT JOIN (
# #                 SELECT
# #                 a1.ID AS Attribute_ID,
# #                 STRING_AGG(CASE WHEN t1.Schema_Name IS NOT NULL THEN t1.Schema_Name END, ' | ') AS Referenced_Schema,
# #                 STRING_AGG(CASE WHEN t1.Table_Name IS NOT NULL THEN t1.Table_Name END, ' | ') AS Referenced_Tables,
# #                 STRING_AGG(CASE WHEN a2.Attribute_Name IS NOT NULL THEN a2.Attribute_Name END, ' | ') WITHIN GROUP (ORDER BY a2.Attribute_Name) AS Referenced_Attributes
# #                 FROM Meta_Attribute_Reference ra
# #                 JOIN Meta_Attribute a1 ON ra.Attribute_ID = a1.id
# #                 JOIN Meta_Attribute a2 ON ra.Reference_Attribute_ID = a2.id
# #                 JOIN Meta_Datastore d1 ON d1.ID = a2.Datastore_ID
# #                 JOIN Meta_Table t1 ON d1.ID = t1.Datastore_ID
# #                 GROUP BY a1.ID
# #                 ) RA ON P.ID = RA.Attribute_ID
# #                 WHERE Agency_Short_Name = '{os.getenv('AGENCY')}'

# #                 AND Dataset_Short_Name = '{os.getenv('DATASET')}'
# #                 and DA.Datastore_Name in ('ossys_Tenant' , 'OSUSR_kzv_AssistedFlareType') -- and P.ATTRIBUTE_NAME !='IS_ACTIVE'
# #                 AND DA.Datastore_Zone = 4
# #                 AND DA.IsActive = 1
# #                 AND T.IsActive = 1
# #                 AND P.IsActive = 1 
# # '''
# #     cmcTableDF=pd.read_sql(query,customMetaDataCatlogDatabase.getConnection())

# #     connectionDF=customMetaDataCatlogDatabase.retriveAConnection(cmcTableDF['CONNECTION_ID'][cmcTableDF.index.min()])
# #     connectionDF=pd.read_sql(query,customMetaDataCatlogDatabase.getConnection())

    # if(os.getenv('DATASET') == 'IGS'):
    customMetaDataCatlogDatabase=Connection(IP='localhost',port= '1433',databaseName='MetadataTracking',password='123456' ,userName='crdetect1',databaseType='SQL')
    print(customMetaDataCatlogDatabase.connect())
    print('testing connection sourceDatabase ',customMetaDataCatlogDatabase.testConnection())
    metadataDF=customMetaDataCatlogDatabase.getMetadataDF()
    # else:
    sourceConnection=Connection(IP='localhost',port= '1521',databaseName='XEPDB1',password='123456' ,userName='loadTest',databaseType='ORCL')
    print(sourceConnection.connect())
    print('testing connection sourceDatabase ',sourceConnection.testConnection())
    metadataDF=sourceConnection.getMetadataDF()

    query=f'''


        SELECT * FROM DBO.CMC_Columns WHERE PIPELINE_ID=31675 '''
    cmcTableDF=pd.read_sql(query,customMetaDataCatlogDatabase.getConnection())

 

    added_columns = metadataDF.merge(
    cmcTableDF[['TABLE_SCHEMA', 'TABLE_NAME', 'PIPELINE_ID']], 
    on=['TABLE_SCHEMA', 'TABLE_NAME'], 
    how='left', 
    indicator=True
    )
    added_columns = added_columns[added_columns['_merge'] == 'left_only'].drop(columns='_merge')
    # print(added_columns)

    deleted_columns = cmcTableDF.merge(
    metadataDF[['TABLE_SCHEMA', 'TABLE_NAME' ]], 
    on=['TABLE_SCHEMA', 'TABLE_NAME'], 
    how='left', 
    indicator=True
    )
    deleted_columns = deleted_columns[deleted_columns['_merge'] == 'left_only'].drop(columns='_merge')


    added_tables = (
    added_columns[['TABLE_SCHEMA', 'TABLE_NAME' , 'TABLE_OBJECT_ID']]
    .drop_duplicates()
    .to_dict('records')
    )

    CR_DETECTION['Add Table'].extend(added_tables)
    CR_DETECTION['Logs']['total_changes'] += len(added_tables)


    deleted_tables = (
    deleted_columns[['TABLE_SCHEMA', 'TABLE_NAME' , 'PIPELINE_ID' , 'DATASTORE_ID']]
    .drop_duplicates()
    .to_dict('records')
    )
    pipe_ids = (
    deleted_columns['PIPELINE_ID']
    .dropna()
    .tolist()
    )

    CR_DETECTION['Delete Table'].extend(deleted_tables)
    CR_DETECTION['Logs']['total_changes'] += len(deleted_tables)
    CR_DETECTION['Impacted Pipliens'] = list(
        dict.fromkeys(CR_DETECTION['Impacted Pipliens'] + pipe_ids)
    )


    common_table_columns = cmcTableDF.merge(
    metadataDF,
    on=['TABLE_SCHEMA', 'TABLE_NAME', 'ATTRIBUTE_NAME'],
    how='outer',
    indicator=True,
    suffixes=('_CMC', '_SOURCE')
    )
    common_table_columns = common_table_columns[
        ~common_table_columns['PIPELINE_ID'].isin(CR_DETECTION['Impacted Pipliens'])
    ]
    
    deleted_common_columns = common_table_columns[common_table_columns['_merge'] == 'left_only'].copy()
    added_columns = common_table_columns[common_table_columns['_merge'] == 'right_only'].copy()
    common_columns = common_table_columns[common_table_columns['_merge'] == 'both'].copy()


    columns=[
        'TABLE_SCHEMA',
        'COLUMN_ID',
        'TABLE_NAME', 
        'ATTRIBUTE_NAME',
        'DATA_TYPE_SOURCE',
        'IS_PK_SOURCE',
        'IS_FK_SOURCE'
    ]
    add_change_records(added_columns, columns, CR_DETECTION , 'Add Attribute')


    
    deleted_columns=[
        'TABLE_SCHEMA',
        'TABLE_NAME',
        'PIPELINE_ID',
        'ATTRIBUTE_NAME',
        'IS_PK_CMC',
        'IS_FK_CMC'
    ]
    add_change_records(deleted_common_columns, deleted_columns, CR_DETECTION , 'Delete Attribute')

    pipe_ids = (
    deleted_common_columns['PIPELINE_ID']
    .dropna()
    .astype(int)
    .tolist()
    )
    CR_DETECTION['Impacted Pipliens'] = list(
        dict.fromkeys(CR_DETECTION['Impacted Pipliens'] + pipe_ids)
    )

    modified_PK = common_table_columns[
    (common_table_columns['DATA_TYPE_CMC'].notna()) & 
    (common_table_columns['DATA_TYPE_SOURCE'].notna()) & 
    (common_table_columns['IS_PK_CMC'] !=  common_table_columns['IS_PK_SOURCE']  )
        ].copy().assign(CHANGE_TYPE='Primary Key')  
    

    pk_columns=[
           'TABLE_SCHEMA',
            'TABLE_NAME',
            'ATTRIBUTE_NAME',
            'IS_PK_CMC',
            'IS_PK_SOURCE',
            'CHANGE_TYPE'
    ]
    add_change_records(modified_PK, pk_columns, CR_DETECTION , 'Change Attribute')


    modified_datatypes = common_columns[
    (common_columns['DATA_TYPE_CMC'].notna()) & 
    (common_columns['DATA_TYPE_SOURCE'].notna()) & 
    (common_columns['DATA_TYPE_CMC'] != common_columns['DATA_TYPE_SOURCE'])
        ].copy().assign(CHANGE_TYPE='Data Type and Length')
    
    
    datatype_columns = [
    'TABLE_SCHEMA',
    'TABLE_NAME',
    'ATTRIBUTE_NAME',
    'DATA_TYPE_CMC',
    'DATA_TYPE_SOURCE',
    'CHANGE_TYPE'
]
    add_change_records(modified_datatypes, datatype_columns, CR_DETECTION , 'Change Attribute')    
    pipeline_ids = modified_datatypes['PIPELINE_ID'].unique().astype(int).tolist()
    existing_ids = set(CR_DETECTION['Impacted Pipliens'])
    new_ids = set(pipeline_ids)

    ids_to_add = new_ids - existing_ids
    CR_DETECTION['Impacted Pipliens'].extend(ids_to_add)
    
    
    json_payload = json.dumps(CR_DETECTION, ensure_ascii=False, indent=2)
    print(json_payload)
    
    

    if CR_DETECTION['Logs']['total_changes'] > 0:
        
        query=f'''
           INSERT INTO [snapshot_detection] ([dataset_id] , [total_objects_captured])
           OUTPUT INSERTED.snapshot_id      
           VALUES ( 1000 , {CR_DETECTION['Logs']['total_changes']}) 

'''
        
        result=pd.read_sql(query,customMetaDataCatlogDatabase.getConnection())
        snapshot_id=int(result['snapshot_id'][0])


        if CR_DETECTION['Add Table']:

            for record in added_tables:
                query = f'''
                   SET NOCOUNT ON
                    INSERT INTO [dbo].[metadata_detection] 
                    ([snapshot_id], [source_object_id], [source_value], [change_type])
                    VALUES ({snapshot_id}, {record['TABLE_OBJECT_ID']}, '{record['TABLE_NAME']}', 'Add Table')
                    SELECT 1 AS A
                '''
                result=pd.read_sql(query,customMetaDataCatlogDatabase.getConnection())

        if CR_DETECTION['Delete Table']:

            for record in deleted_tables:
                query = f'''
                   SET NOCOUNT ON
                    INSERT INTO [dbo].[metadata_detection] 
                    ([snapshot_id], [metadata_cmc_id], [change_type])
                    VALUES ({snapshot_id}, {record['DATASTORE_ID']} , 'Delete Table')
                    SELECT 1 AS A
                '''
                result=pd.read_sql(query,customMetaDataCatlogDatabase.getConnection())

        if CR_DETECTION['Add Attribute']:
            added_columns['COLUMN_ID'] = pd.to_numeric(added_columns['COLUMN_ID'], errors='raise').astype('int64')
            for index, row in added_columns.iterrows():
                query = f'''
                    SET NOCOUNT ON
                    INSERT INTO [dbo].[metadata_detection] 
                    ([snapshot_id], [source_object_id], [source_value], [change_type])
                    VALUES ({snapshot_id}, {row['COLUMN_ID']}, '{row['ATTRIBUTE_NAME']}', 'Add Attribute')
                    SELECT 1 AS A
                '''
                result=pd.read_sql(query,customMetaDataCatlogDatabase.getConnection())

        
        if CR_DETECTION['Delete Attribute']:
            deleted_common_columns['ATTRIBUTE_ID'] = pd.to_numeric(deleted_common_columns['ATTRIBUTE_ID'], errors='raise').astype('int64')
            for index, row in deleted_common_columns.iterrows():
                query = f'''
                    SET NOCOUNT ON
                    INSERT INTO [dbo].[metadata_detection] 
                    ([snapshot_id], [metadata_cmc_id],  [change_type])
                    VALUES ({snapshot_id}, {row['ATTRIBUTE_ID']}, 'Delete Attribute')
                    SELECT 1 AS A
                '''
                result=pd.read_sql(query,customMetaDataCatlogDatabase.getConnection())

        if CR_DETECTION['Change Attribute']:
            modified_PK['COLUMN_ID'] = pd.to_numeric(modified_PK['COLUMN_ID'], errors='raise').astype('int64')
            modified_PK['ATTRIBUTE_ID'] = pd.to_numeric(modified_PK['ATTRIBUTE_ID'], errors='raise').astype('int64')
            for index, row in modified_PK.iterrows():
                query = f'''
                    SET NOCOUNT ON
                    INSERT INTO [dbo].[metadata_detection] 
                    ([snapshot_id], [source_object_id], [metadata_cmc_id] , [source_value], [change_type])
                    VALUES ({snapshot_id}, {row['COLUMN_ID']},  '{row['ATTRIBUTE_ID']}', '{row['IS_PK_SOURCE']}', '{row['CHANGE_TYPE']}')
                    SELECT 1 AS A
                '''
                result=pd.read_sql(query,customMetaDataCatlogDatabase.getConnection())

            modified_datatypes['COLUMN_ID'] = pd.to_numeric(modified_datatypes['COLUMN_ID'], errors='raise').astype('int64')
            modified_datatypes['ATTRIBUTE_ID'] = pd.to_numeric(modified_datatypes['ATTRIBUTE_ID'], errors='raise').astype('int64')
            for index, row in modified_datatypes.iterrows():
                query = f'''
                    SET NOCOUNT ON
                    INSERT INTO [dbo].[metadata_detection] 
                    ([snapshot_id], [source_object_id], [metadata_cmc_id] , [source_value], [change_type])
                    VALUES ({snapshot_id}, {row['COLUMN_ID']},  '{row['ATTRIBUTE_ID']}', '{row['DATA_TYPE_SOURCE']}', '{row['CHANGE_TYPE']}')
                    SELECT 1 AS A
                '''
                result=pd.read_sql(query,customMetaDataCatlogDatabase.getConnection())
            

    
        azureDevops  = AzureDevops(os.getenv('ORGANIZATION'),os.getenv('AREA_PATH'),os.getenv('PAT'),os.getenv('ORGANIZATION'))
        workItems=azureDevops.getWorkItems(os.getenv('AGENCY') , os.getenv('DATASET'))
        print(workItems)
        updateworkItem=azureDevops.updateTicket(TicketID=workItems[0]['id'],TargetColumn=os.getenv('TARGET_COLUMN'),State=None,Tfs_Column=os.getenv('KANBAN_COLUMN'),Comment='Dear DQ Start',Assignee=None )





    import json
import pprint
from classes.Connection import Connection
import re
import os
from requests.auth import HTTPBasicAuth
import requests
from bs4 import BeautifulSoup
import pandas as pd
import base64
from datetime import datetime


class AzureDevops:

    
    def __init__(self,ORGANIZATION=None,PROJECT=None,PAT=None,organization_url=None):
            self.PROJECT=PROJECT
            self.PAT=PAT
            self.ORGANIZATION=ORGANIZATION
            self.organization_url= ORGANIZATION
            self.session = requests.Session()
            self.session.auth = HTTPBasicAuth('', PAT)
            self.session.verify = False  # Disable SSL verification


    def getWorkItems(self,Agency,Dataset):
        
        work_items_data = []
        try:
            wiql_query = {
            'query': f"""
                SELECT [System.Id], [System.Title], [System.State], [System.BoardColumn] , [System.AreaPath], [System.CreatedBy]
                FROM WorkItems
                WHERE [System.AreaPath] = "{self.PROJECT}"
                AND [System.WorkItemType] = 'Feature'
                AND [System.Title] CONTAINS '{Agency} - {Dataset}'
            """
            }

            response = requests.post(
                    f'{self.organization_url}/_apis/wit/wiql?api-version=6.0',
                    json=wiql_query , auth=HTTPBasicAuth("", self.PAT), verify=False
                )
            # response.raise_for_status()  # Check for HTTP error
            wiql_result = response.json()
            if response.status_code !=200 or not wiql_result['workItems']:
                return []
            work_item_ids = [item['id'] for item in wiql_result['workItems']]
            response = self.session.get(
                    f'{self.organization_url}/_apis/wit/workitems?ids={",".join(map(str, work_item_ids))}&api-version=6.0'
                )
            response.raise_for_status()  # Check for HTTP errors
            work_items = response.json()['value']
            

              
            for item in work_items:
                fields = item['fields'] 
                work_item_data = {
                            'id': item['id'],
                            'title': fields.get('System.Title', ''),
                            'boardColumn': fields.get('System.BoardColumn', ''),
                            'workItemType':fields.get('System.WorkItemType', ''),
                            'areaPath':fields.get('System.AreaPath', ''),
                            'CreatedBy': fields.get('System.CreatedBy', '')
                        }
                            
                work_items_data.append(work_item_data)
               
        except Exception as e:
            print(f"An error occurred: {e}")
            
        return work_items_data
    
    def parse_datetime_string(self,datetime_string):
        formats = [
            '%Y-%m-%dT%H:%M:%SZ',
            '%Y-%m-%dT%H:%M:%S.%fZ',   
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(datetime_string, fmt)
            except ValueError:
                continue
        raise ValueError(f"Could not parse datetime string: {datetime_string}")


    
    def generateReport(self):
        work_items_data = []
        try:
                wiql_query = {
                    'query': f"""
                        SELECT [System.Id], [System.Title], [System.State], [System.BoardColumn] , [System.CreatedBy]
                        FROM WorkItems
                        WHERE [System.TeamProject] = 'NDB2 - Data Integration'
                         AND [System.WorkItemType] = 'Feature'
                         AND [System.CreatedDate] > '2024-02-10'


                    """
                }
                time = 0
                response = self.session.post(
                    f'{self.organization_url}/_apis/wit/wiql?api-version=6.0',
                    json=wiql_query
                )
                response.raise_for_status()
                wiql_result = response.json()['workItems']
                work_item_ids = [item['id'] for item in wiql_result]
                schema_updates=[]
                response = self.session.get(
                    f'{self.organization_url}/_apis/wit/workitems?ids={",".join(map(str, work_item_ids))}&api-version=6.0'
                )
                time_diff=0
                work_items = response.json()['value']
                november=datetime(2024,10,24)
                for item in work_items:

                    response = self.session.get(
                    f'{self.organization_url}/_apis/wit/workitems/{item["id"]}/updates?&api-version=6.0'
                )

                    updates = response.json()['value']
                    for update in updates:
                        fields=update.get('fields',{})
                        if 'relations' in update and  'System.ChangedDate' in fields and fields['System.ChangedDate'].get('oldValue') is not None  and self.parse_datetime_string(fields['System.ChangedDate'].get('oldValue')) < november:
                           data=update.get('relations' , ['']).get('added',[{'attributes':{'name':''}}])
                           
                           if  'development' in data[0].get('attributes','').get('name',''):
                                time=((self.parse_datetime_string(fields['System.ChangedDate'].get('newValue'))  - self.parse_datetime_string(fields['System.ChangedDate'].get('oldValue'))).total_seconds())/60
                                time_diff+=time
                                schema_updates.append( {
                                'ID': item['id'],
                                'Title': item.get('fields',{}).get('System.Title', ''),
                                'BoardColumn': fields.get('System.BoardColumn', ''),
                                'ChangedDate': fields.get('System.ChangedDate', ''),
                                'ResponseTime':time
                            })
        except requests.RequestException as e:
            print(f"An error occurred: {e}")
        schema_updates.append({'Average Time':str((time_diff/len(schema_updates)))+' Minutes' })
        return schema_updates

                
                    
    def getBeneficiaryTickets(self,condition):
            work_items_data = []
            try:
                wiql_query = {
                    'query': f"""
                        SELECT [System.Id], [System.Title], [System.State], [System.BoardColumn] , [System.CreatedBy]
                        FROM WorkItems
                        WHERE [System.TeamProject] = '{self.PROJECT}'
                        AND [System.BoardColumn] IN {condition}
                        AND [Custom.IssueType] = 'Outdated Tables'
                    """
                } ## AND [System.Id] in (54242)
                response = self.session.post(
                    f'{self.organization_url}/_apis/wit/wiql?api-version=6.0',
                    json=wiql_query
                )
                response.raise_for_status()  # Check for HTTP errors
                wiql_result = response.json()['workItems']
                work_item_ids = [item['id'] for item in wiql_result]
                
                response = self.session.get(
                    f'{self.organization_url}/_apis/wit/workitems?ids={",".join(map(str, work_item_ids))}&api-version=6.0'
                )
                response.raise_for_status()  # Check for HTTP errors
                work_items = response.json()['value']
                
                for item in work_items:
                    fields = item['fields']
                    
                    if 'custom.Query' in fields:
                        soup = BeautifulSoup(fields['custom.Query'], 'html.parser')
                        queryWithoutHTML = soup.get_text(separator='\n').strip()
                        

                        combined_query = queryWithoutHTML.strip()
               
                      
                       
                        if re.search(r'union all', queryWithoutHTML, re.IGNORECASE):
                            
                            split_queries = re.split(r'union all', queryWithoutHTML, flags=re.IGNORECASE)
                        else:
                            # Split by 'select' (case insensitive)
                            split_queries = re.split(r'(?i)select', queryWithoutHTML)
                            split_queries = [f'SELECT {q.strip()}' for q in split_queries if q.strip()] 
                        
                            
                       
                        query = []
                        for i, q in enumerate(split_queries):
                            
                            ndb2query = q.strip()
                            clean_text = re.sub(r'<.*?>', '', ndb2query)
                            clean_text = clean_text.strip()
                            query.append(clean_text)
                        
                        work_item_data = {
                            'id': item['id'],
                            'title': fields.get('System.Title', ''),
                            'state': fields.get('System.State', ''),
                            'boardColumn': fields.get('System.BoardColumn', ''),
                            'CreatedBy': fields.get('System.CreatedBy', ''),
                            'ndbQuery': query,
                            'ndbQueryCombined': combined_query 
                        }
                            
                        work_items_data.append(work_item_data)
                        query = []
            except requests.RequestException as e:
                print(f"An error occurred: {e}")
            
            return work_items_data


    
    
    def getAnnouncedCrDatasetWorkItems(self):
        work_items_data = []
        try:
            wiql_query = {
            'query': f"""
                SELECT [System.Id], [System.Title], [System.State], [System.BoardColumn] , [System.AreaPath], [System.CreatedBy],[System.WorkItemType],
                [custom.agency],[custom.datasetName]
                FROM WorkItems
                WHERE [System.TeamProject] = 'Batch-Based Data Integration'
                AND [System.WorkItemType] in  ('New Dataset' , 'CR Announcement' )
                AND [System.State] in ('Completed & Send Email Notification')      
                AND [State Change Date] >= @Today - 5
                AND [custom.platform]='NDB2'  

            """
            }
            # ,'Data Re-Initialization')
            response = self.session.post(
                    f'{self.organization_url}/_apis/wit/wiql?api-version=6.0',
                    json=wiql_query
                )
            # response.raise_for_status()  # Check for HTTP error
            wiql_result = response.json()
            if response.status_code !=200 or not wiql_result['workItems']:
                return []
            work_item_ids = [item['id'] for item in wiql_result['workItems']]
            response = self.session.get(
                    f'{self.organization_url}/_apis/wit/workitems?ids={",".join(map(str, work_item_ids))}&$expand=all&api-version=6.0'
                )
             

            # response.raise_for_status()  # Check for HTTP errors
            work_items_json = response.json()
            print("work_items_json:", work_items_json)
            work_items = work_items_json.get('value', [])
            if not work_items:
                print("No 'value' key in response or it's empty.")
                return []

            latest_items = {}
            for item in work_items:
                key = (item["fields"].get("custom.agency", "") + item["fields"].get("custom.datasetName", ""))
                if key not in latest_items or item["id"] > latest_items[key]["id"]:
                    latest_items[key] = item

            for item in latest_items.values():
                fields = item['fields']
                version = None       
                if 'relations' in item:
                    
                    for relation in item['relations']:
                        if relation['rel'] == 'AttachedFile' and 'attributes' in relation and 'name' in relation['attributes']:
                            name = relation['attributes']['name']
                            match = re.search(r'_V\d+R\d+', name)
                            if match:
                                version = match.group()[1:]  # Remove leading underscore
                            
                work_item_data = {
                    'id': item['id'],
                    'title': fields.get('System.Title', ''),
                    'state': fields.get('System.State', ''),
                    'areaPath': fields.get('System.AreaPath', ''),
                    'CreatedBy': fields.get('System.CreatedBy', ''),
                    'workItemType': fields.get('System.WorkItemType', ''),
                    'version': version}
                work_items_data.append(work_item_data)
                        
                    
        except Exception as e:
            print(f"An error occurred: {e}")
        
        return work_items_data
    
    

    

    def updateTicket(self,TicketID,TargetColumn,State,Tfs_Column,Comment,Assignee):
        print('step1')     
        url = f'{self.organization_url}/_apis/wit/workitems/{TicketID}?api-version=6.0-preview.3'
        response = self.session.get(url)
        response.raise_for_status()
        work_item=response.json()


        update_document = []

        if Comment is not None:
            update_document.append({
                'op': 'add',
                'path': '/fields/System.History',
                'value': Comment
            })

        if Assignee is not None:
            update_document.append({
                'op': 'add',
                'path': '/fields/System.AssignedTo',
                'value': Assignee
            })

        if State is not None :
            update_document.append( {
                'op': 'add',
                'path': '/fields/System.State',
                'value': State
            })
            
            
        if TargetColumn is not None :
            update_document.append( {
                'op': 'add',
                'path': f'/fields/{Tfs_Column}',
                'value': TargetColumn
            })
        self.update_work_item(TicketID, update_document)

        return 1

    


    def update_work_item(self, TicketID, update_document):
        url = f'{self.organization_url}/_apis/wit/workitems/{TicketID}?api-version=6.0-preview.3'
        headers = {
            'Content-Type': 'application/json-patch+json'
        }
        response = self.session.patch(url, json=update_document, headers=headers)
        response.raise_for_status()  # Raise error for bad status codes
        return response.json()
    

    
    
    def attachXlsxToWorkItem(self ,filePath,workItemID):
        
        upload_url = f'{self.organization_url}/_apis/wit/attachments?fileName={filePath.split("/")[-1]}&api-version=6.0'

        headers = {
            'Content-Type': 'application/octet-stream'  
        }

        with open(filePath, 'rb') as file_data:
            upload_response =  self.session.post(upload_url, headers=headers, data=file_data, auth=HTTPBasicAuth('', self.PAT) , )

        # Check if the upload was successful
        if upload_response.status_code == 201:
            upload_response_json = upload_response.json()
            attachment_url = upload_response_json['url']
            print(f"File uploaded successfully. Attachment URL: {attachment_url}")
            
            
            work_item_url = f'{self.organization_url}/_apis/wit/workItems/{workItemID}?api-version=6.0'
            
            
            json_body = [
                {
                    "op": "add",
                    "path": "/relations/-",
                    "value": {
                        "rel": "AttachedFile",
                        "url": attachment_url,
                        "attributes": {
                            "comment": "Attached XLSX file"
                        }
                    }
                }
            ]
            
            headers = {
                'Content-Type': 'application/json-patch+json'
            }
            
            # Make the PATCH request to attach the file to the work item
            attach_response =  self.session.patch(work_item_url, json=json_body, headers=headers, auth=HTTPBasicAuth('', self.PAT))
            
            if attach_response.status_code == 200:
                print("File attached successfully to the work item.")
            else:
                print(f"Failed to attach file to work item: {attach_response.status_code} - {attach_response.text}")
        else:
            print(f"File upload failed: {upload_response.status_code} - {upload_response.text}")

    def downloadSchemaMetaData(self,Agency_Code,Dataset_Code,Fixed_UPLOAD_DIR , Item,Tfs_Column):
        base_url=self.organization_url+f'/_apis/git/repositories/4faf5f83-203e-4038-a28c-08765320acf5/itemsbatch?api-version=6.0'
        data = {
        "itemDescriptors": [
            {
                "path": f'/{Agency_Code}/{Dataset_Code}',
                "recursionLevel": "OneLevel" 
            }
            ]
        }
        
        response = self.session.post(base_url, headers={'Content-Type': 'application/json'}, data=json.dumps(data), auth=HTTPBasicAuth("", self.PAT))
        
        if response.status_code != 200:
            updateworkItem=self.updateTicket(Item['id'],'Not Ready for Profiling',None,Tfs_Column,(f"Dear <a href='#' data-vss-mention='version:2.0,{Item['CreatedBy']['id']}'>@{Item['CreatedBy']['displayName']}</a>,<br>" )+
                    ("Following error message appears:<br><br><span style=\"color:rgb(155, 0, 0);background-color:rgb(204, 204, 204)\"><b><b style=\"box-sizing:border-box;outline:none;background-color:rgb(204, 204, 204)\">Agency/Dataset folder doesn't exist or naming mismatch.</b></b>  </span><br>"),
                                                                                Item['CreatedBy']['displayName'])
            return '-1'
        
            
            
            
            
        folderPath=response.json()['value'][0][-1]['path']
        data = {
        "itemDescriptors": [
            {
                "path": f'{folderPath}/Schema',
                "recursionLevel": "OneLevel" 
            }
            ]
        }

    
        response = self.session.post(base_url, headers={'Content-Type': 'application/json'}, data=json.dumps(data), auth=HTTPBasicAuth("", self.PAT))
        if response.status_code != 200:
            updateworkItem=self.updateProfileTicket(Item['id'],'Not Ready for Profiling',None,Tfs_Column,(f"Dear <a href='#' data-vss-mention='version:2.0,{Item['CreatedBy']['id']}'>@{Item['CreatedBy']['displayName']}</a>,<br>" )+
                    ("Following error message appears:<br><br><span style=\"color:rgb(155, 0, 0);background-color:rgb(204, 204, 204)\"><b><b style=\"box-sizing:border-box;outline:none;background-color:rgb(204, 204, 204)\">Schema Folder doesn't exist in latest folder.</b></b>  </span><br>"),
                                                                                Item['CreatedBy']['displayName'])
            return '-1'
        
        
        folder_contents = response.json()['value'][0]
        
        url=f'{self.organization_url}/_apis/git/repositories/4faf5f83-203e-4038-a28c-08765320acf5/refs?filter=heads/main&api-version=6.0'
        response = self.session.get(url,headers={'Content-Type': 'application/json'}, data=json.dumps(data),auth=HTTPBasicAuth("", self.PAT))
        
        url=f'{self.organization_url}/_apis/git/repositories/4faf5f83-203e-4038-a28c-08765320acf5/pushes?api-version=6.0'
        data = {
            "refUpdates": [
                {
                    "name": f"refs/heads/main",
                    "oldObjectId": response.json()['value'][0]['objectId']
                }
            ],
            "commits": [
                {
                    "comment": "Create folder Profiling and add a dummy file",
                    "changes": [
                        {
                            "changeType": "add",
                            "item": {
                                "path":folderPath+'/Profiling/dummy.txt'
                            },
                            "newContent": {
                                "content": 'This is a placeholder file for creating the folder.',
                                "contentType": "rawtext"
                            }
                        }
                    ]
                }
            ]
        }

        
        response = self.session.post(url,headers={'Content-Type': 'application/json'}, data=json.dumps(data),auth=HTTPBasicAuth("", self.PAT))
        downloadPath = None

        
        # Loop through 'value' array to check for .xlsx files
        for item in folder_contents:
            # Check if the item path ends with .xlsx
            if f'{folderPath}/Schema/{Agency_Code}_{Dataset_Code}_{folderPath.split("/")[-1]}.' in item['path']:
                downloadPath = item['path']  # Save the path of the .xlsx file
                break  # Break if you only need the first .xlsx file (remove if you want all)
        if downloadPath is None:
            updateworkItem=self.updateProfileTicket(Item['id'],'Not Ready for Profiling',None,Tfs_Column,(f"Dear <a href='#' data-vss-mention='version:2.0,{Item['CreatedBy']['id']}'>@{Item['CreatedBy']['displayName']}</a>,<br>" )+
                    ("Following error message appears:<br><br><span style=\"color:rgb(155, 0, 0);background-color:rgb(204, 204, 204)\"><b><b style=\"box-sizing:border-box;outline:none;background-color:rgb(204, 204, 204)\">The specified schema name is invalid or dosen't exists.</b></b>  </span><br>"),
                                                                                Item['CreatedBy']['displayName'])
            return '-1'
           
        file_name = os.path.basename(downloadPath)
        downloadurl = self.organization_url+f'/_apis/git/repositories/4faf5f83-203e-4038-a28c-08765320acf5/items?path={downloadPath}&download=true&api-version=6.0'
        print(downloadurl ,'url')
        response = self.session.get(downloadurl, headers={'Content-Type': 'application/json'} ,auth=HTTPBasicAuth("", self.PAT))
        
        # print(file_name,'fileName')
        if response.status_code == 200:
            with open(str(Fixed_UPLOAD_DIR /file_name), 'wb') as file:
                file.write(response.content)
            print(f"File downloaded successfully and saved to {str(Fixed_UPLOAD_DIR /file_name)}!")
        
            
        
        return file_name
    
    

    def uploadSheet(self,Agency_Code,Dataset_Code,file_path):
        base_url=self.organization_url+f'/_apis/git/repositories/4faf5f83-203e-4038-a28c-08765320acf5/itemsbatch?api-version=6.0'
        data = {
        "itemDescriptors": [
            {
                "path": f'/{Agency_Code}/{Dataset_Code}',
                "recursionLevel": "OneLevel" 
            }
            ]
        }
        
        response = self.session.post(base_url, headers={'Content-Type': 'application/json'}, data=json.dumps(data), auth=HTTPBasicAuth("", self.PAT))
        
        if response.status_code != 200:
            return '-1'
        folderPath=response.json()['value'][0][-1]['path']
        data = {
        "itemDescriptors": [
            {
                "path": f'{folderPath}/Schema',
                "recursionLevel": "OneLevel" 
            }
            ]
        }

    
        response = self.session.post(base_url, headers={'Content-Type': 'application/json'}, data=json.dumps(data), auth=HTTPBasicAuth("", self.PAT))
        if response.status_code != 200:
            return '-2'
        
        
        folder_contents = response.json()['value'][0]
        version = re.search(r'V\d+R\d+', folderPath).group()
        
        url=f'{self.organization_url}/_apis/git/repositories/4faf5f83-203e-4038-a28c-08765320acf5/refs?filter=heads/main&api-version=6.0'
        response = self.session.get(url,headers={'Content-Type': 'application/json'}, data=json.dumps(data),auth=HTTPBasicAuth("", self.PAT))
        
        
        
        with open(file_path, "rb") as file:
            content = base64.b64encode(file.read()).decode()
        
        url=f'{self.organization_url}/_apis/git/repositories/4faf5f83-203e-4038-a28c-08765320acf5/pushes?api-version=6.0'
        data = {
            "refUpdates": [
                {
                    "name": f"refs/heads/main",
                    "oldObjectId": response.json()['value'][0]['objectId']
                }
            ],
            "commits": [
                {
                    "comment": "Upload Passed Profiling sheet.",
                    "changes": [
                        {
                            "changeType": "add",
                            "item": {
                                "path":f'{folderPath}/Profiling/{Agency_Code}_{Dataset_Code}_Profiling_{version}.xlsx'
                            },
                            "newContent": {
                                "content":content ,
                                "contentType": "base64encoded"
                            }
                        }
                    ]
                }
            ]
        }

        
        response = self.session.post(url,headers={'Content-Type': 'application/json'}, data=json.dumps(data),auth=HTTPBasicAuth("", self.PAT))
        # response.raise_for_status() 
        return response.json()
    
    
    
    
    
    def uploadSchemaMetaData(self,file_path,file_name,Item,Tfs_Column , con):
        url = os.getenv('URL')+':8443/api/metadata'

        files = {
            'filepond': open(file_path, 'rb'), 
        }


        headers = {
            'Access-Control-Allow-Origin': os.getenv('URL')+':80',
            'Access-Control-Allow-Credentials': 'true',
            'Access-Control-Allow-Methods': 'POST'
        }

        try:
            response = requests.post(url, files=files, headers=headers)
            if(response.status_code==400):
                updateworkItem=self.updateProfileTicket(Item['id'],'Not Ready for Profiling',None,Tfs_Column,(f"Dear <a href='#' data-vss-mention='version:2.0,{Item['CreatedBy']['id']}'>@{Item['CreatedBy']['displayName']}</a>,<br>" )+
                    ('Error in schema:<br><br>')+ (f"<span style=\"color:rgb(155, 0, 0);background-color:rgb(204, 204, 204)\"><b><b style=\"box-sizing:border-box;outline:none;background-color:rgb(204, 204, 204)\">{response.json()[0]}</span><br>"),
                                                                                Item['CreatedBy']['displayName'])
                return '-1'
            return '1'
        except Exception as e:
            print(e)
            q=f'''	select Error_Msg from   [CONFIG].[Agency_Schema_Source_Log] where Row_ID=(

        select max(Row_ID) from [CONFIG].[Agency_Schema_Source_Log] ag left join [CONFIG].[Agency_Schema_Source] a on a.Agency_Schema_Source_ID=ag.Agency_Schema_Source_ID
        where a.[File_Name] = '{file_name}'

        )'''
            a=pd.read_sql(q,con)
            updateworkItem=self.updateProfileTicket(Item['id'],'Not Ready for Profiling',None,Tfs_Column,(f"Dear <a href='#' data-vss-mention='version:2.0,{Item['CreatedBy']['id']}'>@{Item['CreatedBy']['displayName']}</a>,<br>" )+
                    ('Error in schema:<br><br>')+ (f"<span style=\"color:rgb(155, 0, 0);background-color:rgb(204, 204, 204)\"><b><b style=\"box-sizing:border-box;outline:none;background-color:rgb(204, 204, 204)\">{ a['Error_Msg'][0]}</span><br>"),
                                                                                Item['CreatedBy']['displayName'])
            return '-1'
    
    


import json
import oracledb
import pandas as pd
import cx_Oracle
import requests
import sqlalchemy
import urllib
from sqlalchemy.orm import sessionmaker
from datetime import datetime
import pyodbc
import os
import socket
import pymysql
import psycopg2
import pymssql
import sys
import re
from urllib.parse import quote_plus
currenttime = datetime.today()

dbname=os.getenv('DB_NAME')
dbpass=os.getenv('DB_PASSWORD')
dbuser=os.getenv('DB_USERNAME')
dbip=os.getenv('DB_IP')
dbport=os.getenv('DB_PORT')
schemaip=os.getenv('SCEHMA_IP')
schemaport=os.getenv('SCEHMA_PORT')

print('in connection class ',dbname,dbpass,dbuser,dbip,dbport,schemaip,schemaport , sys.version)

class Connection:


    def __init__(self):
        # self.tableID=Metadata(tableID)
        self.option=None
        self.connection=None

    def __init__(self,IP=None,port=None,databaseName=None,password=None,description=None,userName=None,TNSNAME=None,databaseType=None,connectionType=None,oracleDBType=None,API=None,option=None,MetaData_UPLOAD_DIR=None,odbcName=None):
        # self.tableID=Metadata(tableID)
        self.option=option
        self.connection=None
        self.IP=IP
        self.port=port
        self.databaseName=databaseName
        self.password=password
        self.description=description
        self.userName=userName
        self.TNSNAME=TNSNAME
        self.databaseType=databaseType
        self.connectionType=connectionType
        self.oracleDBType=oracleDBType
        self.API=API
        self.MetaData_UPLOAD_DIR=MetaData_UPLOAD_DIR
        self.odbcName=odbcName

    
    

    def connect(self ):
        if(self.option=='Destination'):
            params = urllib.parse.quote_plus(
                f"DRIVER={{ODBC Driver 17 for SQL Server}};"
                f"SERVER={dbip};"
                f"DATABASE={dbname};"
                f"UID={dbuser};"
                f"PWD={dbpass};"
            )
                    

            self.connection = sqlalchemy.create_engine(
                f"mssql+pyodbc:///?odbc_connect={params}",
                fast_executemany=True,
                connect_args={'connect_timeout': 10},
                pool_pre_ping=True  # Optional: test connections before using them
            ) 
        elif(self.option=='ndb2'):
            # temp=connect(host = "localhost", port = 21050)
            self.connection=pyodbc.connect(DSN=self.odbcName, autocommit=True, encoding='utf-8')
        elif(self.option=='40'):
            params = urllib.parse.quote_plus("DRIVER={ODBC+Driver+17+for+SQL+Server};"
                                 "SERVER="+'10.9.8.40'+","+'2995'+";"
                                 "DATABASE="+'NICDATA'+";"
                                 "UID="+'sa'+";"
                                 "PWD="+'Nic@estishraf2030')
        

            

            self.connection=sqlalchemy.create_engine("mssql+pyodbc:///?odbc_connect={}".format(params),
            execution_options={
                "isolation_level": "AUTOCOMMIT"
            },encoding="utf8",fast_executemany = True,connect_args={'connect_timeout': 10}) 
        else:
            if(self.databaseType=='SQL'):
                msg=0

                if(self.databaseName.strip()==''):
                    msg='You have to provide database name!!'
                    return msg
                elif(';' in self.databaseName):
                    msg='Is it valid that database name contains '';'' ?'
                    return msg

                try:
                    serverAndport=f"{self.IP}','{self.port};ApplicationIntent=ReadOnly"
                    self.connection = pymssql.connect(server=self.IP, port=self.port, user=self.userName,
                                                      password=self.password, database=self.databaseName)
                    
                
        
            
            
                except Exception as e:
                    msg=str(e)
                
                return msg
            elif(self.databaseType=='ORCL'):
                msg=0
                print(self.option
                ,self.IP
                ,self.port
                ,self.databaseName
                ,self.password
                ,self.description
                ,self.userName
                ,self.TNSNAME
                ,self.databaseType
                ,self.connectionType
                ,self.connectionType=='2'
                ,self.oracleDBType
                ,self.MetaData_UPLOAD_DIR)

                try:
                    if(self.connectionType=='2'):
                        self.connection = cx_Oracle.connect(self.userName,str(self.password) , self.TNSNAME, encoding="UTF-8")
                    else:
                        self.connection = oracledb.connect(
                        user=self.userName,
                        password=str(self.password) ,
                        dsn=f"""{self.IP}:{self.port}/{self.databaseName}"""
                        )
                        # self.

            
                except Exception as e:
                    msg=str(e)
                
                return msg
            elif(self.databaseType=='DENODO'):
                msg=0 
                print(self.option
                ,self.IP
                ,self.port
                ,self.databaseName
                ,self.password
                ,self.description
                ,self.userName
                ,self.TNSNAME
                ,self.databaseType
                ,self.connectionType
                ,self.connectionType=='2'
                ,self.oracleDBType
                ,self.MetaData_UPLOAD_DIR) 

                
                try:
              
                    self.connection = psycopg2.connect("user=%s password=%s host=%s dbname=%s port=%s" %\
                        (self.userName, self.password, self.IP, self.databaseName, self.port))
                     
                except Exception as e:
                    msg=str(e)      
                
                return msg
            elif(self.databaseType=='POSTGRESQL'):
                msg=0 
                print(self.option
                ,self.IP
                ,self.port
                ,self.databaseName
                ,self.password
                ,self.description
                ,self.userName
                ,self.TNSNAME
                ,self.databaseType
                ,self.connectionType
                ,self.connectionType=='2'
                ,self.oracleDBType
                ,self.MetaData_UPLOAD_DIR) 
                try:

                    serverAndport=self.IP+':'+self.port

                    self.connection=sqlalchemy.create_engine("postgresql+psycopg2://"""+self.userName+""":"""+self.password+"""@"""+serverAndport+"""/"""+self.databaseName,
                    execution_options={
                        "isolation_level": "AUTOCOMMIT"
                    },encoding="utf8")
        
                except Exception as e:
                    msg=str(e)
                
                return msg
            elif(self.databaseType=='MySQL'):
                print(self.option
                ,self.IP
                ,self.port
                ,self.databaseName
                ,self.password
                ,self.description
                ,self.userName
                ,self.TNSNAME
                ,self.databaseType
                ,self.connectionType
                ,self.connectionType=='2'
                ,self.oracleDBType
                ,self.MetaData_UPLOAD_DIR)
                msg=0

                try:
                    serverAndport=self.IP+':'+self.port
        
                    encoded_password = quote_plus(self.password)

                    self.connection=sqlalchemy.create_engine("""mysql+pymysql://"""+self.userName+""":"""+encoded_password+"""@"""+serverAndport+"""/"""+self.databaseName+"""?charset=utf8""",
                    execution_options={
                        "isolation_level": "AUTOCOMMIT"
                    },encoding="utf8")
                    

  
            
            
                except Exception as e:
                    msg=str(e)
                
                return msg

            elif(self.databaseType=='Impala'):
                
                msg=0

                try:
                    # serverAndport=self.IP+':'+self.port
                    # params = urllib.parse.quote_plus("DRIVER={MySQL ODBC 8.0 Unicode Driver};"
                    #                 "SERVER="+serverAndport+";"
                    #                 "DATABASE="+self.databaseName+";"
                    #                 "UID="+self.userName+";"
                    #                 "PWD="+self.password)
        

                    # self.connection = sqlalchemy.create_engine("apacheimpala:///?Server=127.0.0.1&;Port=21050")
                    self.connection=pyodbc.connect(DSN=self.odbcName, autocommit=True, encoding='utf-8')
                    # self.connection=sqlalchemy.create_engine("impala:///?Server=127.0.0.1&;Port=21050",
                    # execution_options={
                    #     "isolation_level": "AUTOCOMMIT"
                    # },encoding="utf8")
                    


            
            
                except Exception as e:
                    msg=str(e)
                
                return msg
            else:
                msg='Invalid Connection Type!'  
                return msg                
         

    def getConnection(self):
        return self.connection
    
    
    
    def testConnection(self):
        if(self.databaseType=='SQL' or self.databaseType=='MySQL' or  self.databaseType=='POSTGRESQL' or self.databaseType=='DENODO'):
            msg=0
            try:
                df=pd.read_sql('select 1 as a ',self.connection )
                      
            except Exception as e:
                msg=str(e)
            return msg
        elif(self.databaseType=='ORCL'):
            msg=0
            try:
                df=pd.read_sql('''select 1 AS "a" from dual''',self.connection)
            
            except Exception as e:
                msg=str(e)
            return msg
        elif(self.databaseType=='Impala'):
            msg=0
            return msg
        else:
            msg='Invalid Database Type!'
            return msg
    

    def close(self):
        self.connection.close()    

    def telnet(self):
        msg=''
        if(self.connectionType=='2' or self.connectionType=='3' ): # TNS or API
            msg=0
        else:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            try:
                msg = sock.connect_ex((self.IP,int(self.port)))

                
            except Exception as e:
                msg = str(e)

            sock.close()
        return msg

    

    def getAllConnections(self):
        query="""
        
  
        
        EXEC	[dbo].[PR_Get_Connection]
        @EncryptionKey	= 'P@ssw0rd@@ndb??123DqteamIsThatComplex'

        """
        a=pd.read_sql(query,self.connection)
        a=a.astype(str)

        return a

    def retriveAConnection(self,Connection_ID):

        query=f"""
        select * from {os.getenv('CONNECTION_TABLE')} where ID = {Connection_ID}

        """
        a=pd.read_sql(query,self.connection)
        

        return a        

 
    
   

    def getMetadataDF(self):
        if(self.databaseType=='SQL'):
            msg=0
            try:
                query='''   SELECT
    OBJECT_ID(cols.TABLE_SCHEMA + '.' + cols.TABLE_NAME) AS TABLE_OBJECT_ID,
    COLUMNPROPERTY(OBJECT_ID(cols.TABLE_SCHEMA + '.' + cols.TABLE_NAME), cols.COLUMN_NAME, 'ColumnId') AS COLUMN_ID,
    TABLE_SCHEMA,
    TABLE_NAME,
    COLUMN_NAME ATTRIBUTE_NAME,
    SCHEMA_NAME(objs.schema_id) BASE_SCHEMA,
    OBJECT_NAME(dpnds.referenced_major_id) BASE_TABLE,
    real_names.name BASE_COLUMN,
    CASE
        WHEN cols.DATA_TYPE IN ('bigint', 'int', 'smallint', 'tinyint', 'bit', 'ntext', 'money', 'smallmoney', 'date', 'datetime', 'smalldatetime', 'uniqueidentifier') THEN DATA_TYPE
        WHEN cols.DATA_TYPE IN ('numeric', 'decimal') THEN DATA_TYPE + ' (' + CONVERT(varchar(50), NUMERIC_PRECISION) + ', ' + CONVERT(varchar(50), NUMERIC_SCALE) + ')'
        WHEN cols.DATA_TYPE IN ('float') THEN DATA_TYPE + ' (' + CONVERT(varchar(50), NUMERIC_PRECISION) + ')'
        WHEN cols.DATA_TYPE IN ('datetime2', 'datetimeoffset', 'time') THEN DATA_TYPE + ' (' + CONVERT(varchar(50), DATETIME_PRECISION) + ')'
        WHEN cols.DATA_TYPE IN ('char', 'varchar', 'text', 'nchar', 'nvarchar', 'binary', 'varbinary', 'image') AND CHARACTER_MAXIMUM_LENGTH != -1 THEN DATA_TYPE + ' (' + CONVERT(varchar(50), CHARACTER_MAXIMUM_LENGTH) + ')'
        WHEN cols.DATA_TYPE IN ('char', 'varchar', 'text', 'nchar', 'nvarchar', 'binary', 'varbinary', 'image') AND CHARACTER_MAXIMUM_LENGTH = -1 THEN DATA_TYPE + ' (max)'
    ELSE cols.DATA_TYPE
    END AS DATA_TYPE,
    -- Check to see if the attribute is in the list of KEYs of this table (Could be composite key)
    IIF(cols.COLUMN_NAME IN 
    (
        SELECT k.column_name
        FROM information_schema.table_constraints t
            JOIN information_schema.key_column_usage k
            ON t.constraint_name = k.constraint_name
        WHERE t.constraint_type='PRIMARY KEY'
            AND t.table_schema=cols.TABLE_SCHEMA
            AND t.table_name=cols.TABLE_NAME
    ), 'Yes', 'No') IS_PK,
    -- FK attributes will depend on the subquery join
    IIF(FK_INFO.FK_SCHEMA IS NOT NULL, 'Yes', 'No') IS_FK,
    IIF(FK_INFO.FK_SCHEMA IS NOT NULL, FK_INFO.Referenced_Schema, '') REFERENCED_SCHEMA,
    IIF(FK_INFO.FK_TABLE IS NOT NULL, FK_INFO.Referenced_Table, '') REFERENCED_TABLE,
    IIF(FK_INFO.FK_COLUMN IS NOT NULL, FK_INFO.Referenced_Column, '') REFERENCED_COLUMN,
    CASE WHEN real_names.name IS NOT NULL AND vws.type_desc IS NOT NULL THEN 'Yes' ELSE 'No' END AS IS_VIEW
FROM INFORMATION_SCHEMA.COLUMNS cols
    -- The following join to get the FK reference
    LEFT JOIN (
        SELECT
            KF.TABLE_SCHEMA FK_SCHEMA,
            KF.TABLE_NAME FK_TABLE,
            KF.COLUMN_NAME FK_COLUMN,
            KP.TABLE_SCHEMA Referenced_Schema,
            KP.TABLE_NAME Referenced_Table,
            KP.COLUMN_NAME Referenced_Column
        FROM INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS RC
            JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE KF ON RC.CONSTRAINT_NAME = KF.CONSTRAINT_NAME
            JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE KP ON RC.UNIQUE_CONSTRAINT_NAME = KP.CONSTRAINT_NAME
    ) AS FK_INFO
    ON  cols.TABLE_SCHEMA = FK_INFO.FK_SCHEMA 
    AND cols.TABLE_NAME = FK_INFO.FK_TABLE 
    AND cols.COLUMN_NAME = FK_INFO.FK_COLUMN
    -- The following joins to get the view base
    LEFT JOIN sys.views vws ON OBJECT_ID(cols.TABLE_SCHEMA + '.' + cols.TABLE_NAME) = vws.object_id
    LEFT JOIN sys.sql_dependencies dpnds ON OBJECT_ID(cols.TABLE_SCHEMA + '.' + cols.TABLE_NAME) = dpnds.object_id AND cols.ORDINAL_POSITION = dpnds.referenced_minor_id
    LEFT JOIN sys.columns real_names ON dpnds.referenced_major_id = real_names.object_id AND dpnds.referenced_minor_id = real_names.column_id
    LEFT JOIN sys.objects objs ON dpnds.referenced_major_id = objs.object_id
WHERE
    TABLE_SCHEMA NOT IN ('db_accessadmin', 'db_backupoperator', 'db_datareader', 'db_datawriter', 'db_ddladmin', 'db_denydatareader', 'db_denydatawriter', 'db_owner',
                        'db_securityadmin', 'db_ssisadmin', 'db_ssisltduser', 'db_ssisoperator', 'guest', 'managed_backup', 'smart_admin', 'SQLAgentOperatorRole',        
                        'SQLAgentReaderRole', 'SQLAgentUserRole', 'TargetServersRole', 'sys', 'INFORMATION_SCHEMA')
                        AND TABLE_NAME in ('ossys_Tenant' , 'ossys_Tenant1') and TABLE_NAME+''+COLUMN_NAME !='ossys_TenantLastOperation'
                            '''
                # filename= self.description+"_"+self.databaseType +"_"+self.databaseName+"_"+currenttime.strftime('%Y%m%d%H%M%S')+".xlsx"
                df=pd.read_sql(query,self.connection)
                # df.to_excel(str( self.MetaData_UPLOAD_DIR / filename),sheet_name="sheet1",index=False)
                return df

            
            except Exception as e:
                msg=str(e)
            return msg
        elif(self.databaseType=='ORCL'):
            msg=0
            try:
                query=f'''
                       SELECT
            main.OWNER AS TABLE_SCHEMA,
            TABLE_NAME,
            COLUMN_NAME AS ATTRIBUTE_NAME,
            mview.DETAILOBJ_OWNER AS BASE_SCHEMA,
            mview.DETAILOBJ_NAME AS BASE_TABLE,
            mview.DETAILOBJ_COLUMN AS BASE_COLUMN,
            CASE
                WHEN DATA_TYPE = 'NUMBER' AND DATA_PRECISION IS NOT NULL AND DATA_SCALE IS NOT NULL THEN 'NUMBER (' || DATA_PRECISION || ', ' || DATA_SCALE || ')'
                WHEN DATA_TYPE = 'FLOAT' AND DATA_PRECISION IS NOT NULL THEN 'FLOAT ' || '(' || DATA_PRECISION || ')'
                WHEN DATA_TYPE IN ('LONG', 'DATE', 'BINARY_FLOAT', 'BINARY_DOUBLE', 'LONG RAW','ROWID', 'UROWID', 'CLOB', 'NCLOB', 'BLOB', 'BFILE') THEN DATA_TYPE
                WHEN DATA_TYPE IN ('VARCHAR', 'VARCHAR2', 'NVARCHAR2') AND CHAR_USED = 'B' THEN DATA_TYPE || ' (' || CHAR_LENGTH || ' BYTE)'
                WHEN DATA_TYPE IN ('VARCHAR','VARCHAR2', 'NVARCHAR2') AND CHAR_USED = 'C' THEN DATA_TYPE || ' (' || CHAR_LENGTH || ' CHAR)'
                WHEN DATA_TYPE IN ('NCHAR', 'CHAR') THEN DATA_TYPE || ' (' || CHAR_LENGTH || ')'
                WHEN DATA_TYPE LIKE 'TIMESTAMP%' THEN DATA_TYPE
                ELSE DATA_TYPE
            END AS DATA_TYPE,
            CASE (SELECT
                    ALL_CONSTRAINTS.CONSTRAINT_TYPE
                  FROM
                    ALL_CONSTRAINTS, ALL_CONS_COLUMNS
                  WHERE
                    UPPER(ALL_CONSTRAINTS.OWNER) = '{self.userName.upper()}'
                    and ALL_CONSTRAINTS.CONSTRAINT_TYPE = 'P'
                    and ALL_CONSTRAINTS.CONSTRAINT_NAME = ALL_CONS_COLUMNS.CONSTRAINT_NAME
                    and ALL_CONSTRAINTS.OWNER = ALL_CONS_COLUMNS.OWNER
                    and ALL_CONS_COLUMNS.TABLE_NAME = main.TABLE_NAME
                    and ALL_CONS_COLUMNS.COLUMN_NAME = main.COLUMN_NAME
                    AND ROWNUM <= 1)
            WHEN 'P' THEN 'Yes'
            ELSE 'No' END IS_PK,
            CASE (SELECT
                COUNT(c_pk.table_name) count
              FROM
                all_cons_columns a
              JOIN all_constraints c ON a.owner = c.owner
                AND a.constraint_name = c.constraint_name
              JOIN all_constraints c_pk ON c.r_owner = c_pk.owner
                AND c.r_constraint_name = c_pk.constraint_name
              WHERE
                UPPER(a.OWNER) = '{self.userName.upper()}'
                AND c.constraint_type = 'R'
                AND a.TABLE_NAME = COALESCE(mview.DETAILOBJ_NAME, main.TABLE_NAME)
                AND a.COLUMN_NAME = COALESCE(mview.DETAILOBJ_COLUMN, main.COLUMN_NAME))
            WHEN 0 THEN 'No'
            ELSE 'Yes' END IS_FK,
            COALESCE((SELECT
                c.R_OWNER
              FROM
                all_cons_columns a
              JOIN all_constraints c ON a.owner = c.owner
                AND a.constraint_name = c.constraint_name
              JOIN all_constraints c_pk ON c.r_owner = c_pk.owner
                AND c.r_constraint_name = c_pk.constraint_name
              WHERE
                UPPER(a.OWNER) = '{self.userName.upper()}'
                AND c.constraint_type = 'R'
                AND a.TABLE_NAME = COALESCE(mview.DETAILOBJ_NAME, main.TABLE_NAME)
                AND a.COLUMN_NAME = COALESCE(mview.DETAILOBJ_COLUMN, main.COLUMN_NAME)
                AND ROWNUM <= 1), ' ') REFERENCED_SCHEMA,
            COALESCE((SELECT
                c_pk.table_name r_table_name
              FROM
                all_cons_columns a
              JOIN all_constraints c ON a.owner = c.owner
                AND a.constraint_name = c.constraint_name
              JOIN all_constraints c_pk ON c.r_owner = c_pk.owner
                AND c.r_constraint_name = c_pk.constraint_name
              WHERE
                UPPER(a.OWNER) = '{self.userName.upper()}'
                AND c.constraint_type = 'R'
                AND a.TABLE_NAME = COALESCE(mview.DETAILOBJ_NAME, main.TABLE_NAME)
                AND a.COLUMN_NAME = COALESCE(mview.DETAILOBJ_COLUMN, main.COLUMN_NAME)
                AND ROWNUM <= 1), ' ') REFERENCED_TABLE,
            COALESCE((SELECT ucc_p.column_name
                        FROM
                            ALL_CONSTRAINTS uc_r
                        JOIN
                            ALL_CONS_COLUMNS ucc_r ON ucc_r.CONSTRAINT_NAME = uc_r.CONSTRAINT_NAME
                        JOIN
                            ALL_CONSTRAINTS uc_p ON uc_p.CONSTRAINT_NAME = uc_r.R_CONSTRAINT_NAME
                        JOIN
                            ALL_CONS_COLUMNS ucc_p ON ucc_p.CONSTRAINT_NAME = uc_p.CONSTRAINT_NAME AND ucc_p.POSITION = ucc_r.POSITION
                        WHERE
                        UPPER(uc_r.OWNER) = '{self.userName.upper()}'
                        AND uc_r.constraint_type = 'R'
                        AND uc_r.TABLE_NAME = COALESCE(mview.DETAILOBJ_NAME, main.TABLE_NAME)
                        AND ucc_r.COLUMN_NAME = COALESCE(mview.DETAILOBJ_COLUMN, main.COLUMN_NAME)
                        AND ROWNUM <= 1), ' ') REFERENCED_COLUMN,
            CASE
              WHEN objects.object_type IN ('VIEW') THEN 'Yes' ELSE 'No' END AS IS_VIEW
        FROM
            ALL_TAB_COLUMNS main
            LEFT JOIN
            SYS.ALL_MVIEW_KEYS mview ON main.OWNER = mview.OWNER AND main.TABLE_NAME = mview.MVIEW_NAME AND main.COLUMN_NAME = mview.CONTAINER_COLUMN
            LEFT JOIN 
            all_objects objects ON main.OWNER = objects.OWNER AND main.TABLE_NAME = objects.object_name
        WHERE
            UPPER(main.OWNER) = '{self.userName.upper()}' -- Replace with schema name
                    '''
                
                df=pd.read_sql(query,self.connection)
                print('Hey')
                print(df)
                return df
            
            except Exception as e:
                msg=str(e)
            return msg
        # elif(self.databaseType=='MySQL'):
        #     msg=0
        #     try:
        #         query='''
        #             SELECT
		# 					{file_ID} as Agency_Schema_Source_ID,
		# 					'test' as agencyName,
		# 					'dataset' as datasetName,
		# 					' ' as datasetDescription,
		# 					'SQL_SERVER' as databaseType,
		# 					'Weekly' as updateFrequency,
		# 					'1' as schemaVersion,
        #             COLUMNS.TABLE_SCHEMA                                                                                            AS 'schemaName',
        #             COLUMNS.TABLE_NAME                                                                                              AS 'tableName',
		# 			' '																												AS 'tableDescription',
		# 			0																												AS 'isLookup',
        #             COLUMNS.COLUMN_NAME                                                                                             AS 'columnName',
		# 			''																												AS 'columnDescription',
        #             COLUMNS.COLUMN_TYPE                                                                                             AS 'dataType',
        #             IF((COLUMNS.COLUMN_KEY = 'PRI'), 1, 0)																			AS 'isPrimaryKey',
		# 			0																												AS 'isLastOperation',
		# 			0																												AS 'isSyncTimestamp',
		# 			''																												AS 'timestampFormat',
		# 			''																												AS 'sampleValues1',
		# 			''																												AS 'sampleValues2',
		# 			''																												AS 'sampleValues3',
		# 			''																												AS 'sampleValues4',
		# 			''																												AS 'sampleValues5',
        #             IF((FORIEGN_KEYS.REFERENCED_TABLE_SCHEMA IS NOT NULL), FORIEGN_KEYS.REFERENCED_TABLE_SCHEMA, ' ')               AS 'referencedSchema',
        #             IF((FORIEGN_KEYS.REFERENCED_TABLE_NAME IS NOT NULL), FORIEGN_KEYS.REFERENCED_TABLE_NAME, ' ')                   AS 'referencedTable',
        #             IF((FORIEGN_KEYS.REFERENCED_COLUMN_NAME IS NOT NULL), FORIEGN_KEYS.REFERENCED_COLUMN_NAME, ' ')                 AS 'referencedColumn',
        #             0                                                                                                            AS 'isAdditional',
		# 			'NO' AS 'maskingValue',
		# 					1 as 'Inserted_By',
		# 					CURDATE() as 'Insert_Timestamp',
		# 					NULL as 'Updated_By',
		# 					NULL as 'Update_Timestamp'
        #             FROM
        #             INFORMATION_SCHEMA.COLUMNS COLUMNS
        #                 LEFT JOIN
        #             (SELECT
        #                 TABLE_CONSTRAINTS.TABLE_SCHEMA,
        #                 TABLE_CONSTRAINTS.TABLE_NAME,
        #                 COLUMN_USAGE.COLUMN_NAME,
        #                 COLUMN_USAGE.REFERENCED_TABLE_SCHEMA,
        #                 COLUMN_USAGE.REFERENCED_TABLE_NAME,
        #                 COLUMN_USAGE.REFERENCED_COLUMN_NAME
        #                 FROM
        #                     INFORMATION_SCHEMA.TABLE_CONSTRAINTS TABLE_CONSTRAINTS
        #                         INNER JOIN
        #                     INFORMATION_SCHEMA.KEY_COLUMN_USAGE COLUMN_USAGE
        #                         ON
        #                     TABLE_CONSTRAINTS.CONSTRAINT_NAME = COLUMN_USAGE.CONSTRAINT_NAME
        #                 WHERE
        #                     CONSTRAINT_TYPE='FOREIGN KEY') FORIEGN_KEYS
        #                 ON
        #                     FORIEGN_KEYS.TABLE_SCHEMA = COLUMNS.TABLE_SCHEMA AND
        #                     FORIEGN_KEYS.TABLE_NAME = COLUMNS.TABLE_NAME AND
        #                     FORIEGN_KEYS.COLUMN_NAME = COLUMNS.COLUMN_NAME
        #             WHERE
        #                 COLUMNS.TABLE_SCHEMA NOT IN ('information_schema', 'mysql', 'performance_schema', 'sys')
        #             ORDER BY
        #                 COLUMNS.TABLE_SCHEMA,
        #                 COLUMNS.TABLE_NAME,
        #                 COLUMNS.COLUMN_NAME;'''.format_map({"file_ID":file_ID})
        #         # filename= self.description+"_"+self.databaseType +"_"+self.databaseName+"_"+currenttime.strftime('%Y%m%d%H%M%S')+".xlsx"
        #         df=pd.read_sql(query,self.connection)
        #         # df.to_excel(str( self.MetaData_UPLOAD_DIR / filename),sheet_name="sheet1",index=False)
        #         return df

        #     except Exception as e:
        #         msg=str(e)
        #     return msg
        else:
            '2'

        return '1'
    
    
    
    
    
    def checkConnectionStatus(self,Connection_ID,con ):
        
        message='Connection Error:'
        # there is no connection added to dataset
        if Connection_ID is None:              
            return message+'Dataset has no connection, Please provide connection details'
        
        
        
        query=f'''  EXEC	[dbo].[PR_Get_Connection]
        @EncryptionKey	= 'P@ssw0rd@@ndb??123Dqteam'
        ,@CONNECTION_ID={Connection_ID}'''
        connection=pd.read_sql(query,con )
        sourceConnection=Connection(IP=connection['IP'][0],port=connection['port'][0],databaseName=connection['DB_Name'][0],password=connection['Password'][0],userName=connection['User_Name'][0],TNSNAME=connection['TNS_Name'][0],databaseType=connection['DB_Type'][0],connectionType=connection['Connection_Type'][0],oracleDBType=connection['Oracle_DB_Type'][0],API=connection['API'][0])
        isConnectionRefused=sourceConnection.telnet()
        
        print('telnet msg',isConnectionRefused)
        if(isConnectionRefused!=0):            
            return isConnectionRefused
        
        connect=sourceConnection.connect()
        print('connect msg',connect)

        if(connect!=0):
            
            if 'login failed' in connect.lower() or 'tcp provider: error code 0x68 (104)%' in connect.lower():
                    message ='login failed'
            elif 'tcp provider: error code 0x2746%' in connect.lower():
                message = message+'No Network'
            elif 'login timeout' in connect.lower():
                message =message+'Login timeout'
            elif 'permission was denied' in connect.lower():
                message =message+'no permission'
            else:
                message =message+'unknown issue'
                        
                                         
            return message
        
        
        if(connection['Connection_Type'][0] != '3'):
            testConnection=sourceConnection.testConnection()
            print('connect msg',testConnection)

            if(testConnection!=0):
                return message
            
        return '1'
    
    
    
    def single_quote(self,word):
        word=str(word).replace("'","''")
        return """'%s'""" % word
